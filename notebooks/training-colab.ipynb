{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 4569,
     "status": "ok",
     "timestamp": 1614858323090,
     "user": {
      "displayName": "童祺俊",
      "photoUrl": "",
      "userId": "10868036175973256652"
     },
     "user_tz": -540
    },
    "id": "8YzFB8sn7coV"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification, AdamW\n",
    "from tqdm import trange,tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import dataloader\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "df=pd.read_csv(\"atcoder_problem_tag_dataset.csv\")\n",
    "\n",
    "df=df[df[\"problem_texts\"].notna()].reset_index()\n",
    "df[\"concatenate_texts\"]=df[\"problem_texts\"]+df[\"constraints\"]+df[\"input_texts\"]+df[\"output_texts\"]\n",
    "le = LabelEncoder()\n",
    "df[\"tag\"] = le.fit_transform(df[\"tag\"])\n",
    "\n",
    "df_copy = df.copy()\n",
    "df_train = df_copy.sample(frac=0.8, random_state=0)\n",
    "df_eval = df_copy.drop(df_train.index).reset_index()\n",
    "df_train = df_train.reset_index()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-cased', return_dict=True,num_labels=14)\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "class DataSet:\n",
    "    def __init__(self,df,tokenizer,input_col_name,output_col_name):\n",
    "        self.X = tokenizer.batch_encode_plus(df[\"concatenate_texts\"].tolist(),truncation=True,max_length=512,pad_to_max_length=True, add_special_tokens=True,return_tensors='pt')\n",
    "        self.y = torch.tensor(df[output_col_name])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return self.X[\"input_ids\"][index].to(device),self.X[\"token_type_ids\"][index].to(device), self.X[\"attention_mask\"][index].to(device), self.y[index].to(device)\n",
    "\n",
    "dataset_train = DataSet(df_train,tokenizer,\"concatenate_texts\",\"tag\")\n",
    "dataset_eval = DataSet(df_train,tokenizer,\"concatenate_texts\",\"tag\")\n",
    "trainset = dataloader.DataLoader(dataset = dataset_train, shuffle=True, batch_size = 8)\n",
    "evalset = dataloader.DataLoader(dataset = dataset_eval, shuffle=True, batch_size = 8)\n",
    "\n",
    "for epoch in trange(1,51):\n",
    "    model.train()\n",
    "    total_loss_train=0\n",
    "    for batch in trainset:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids, input_token_type_ids, input_attention_mask ,labels= batch \n",
    "        output = model(input_ids,token_type_ids = input_token_type_ids,attention_mask =input_attention_mask, labels=labels)\n",
    "        loss = output[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "        total_loss_train+=loss\n",
    "    print(\"epoch\",epoch)\n",
    "    print(\"    Avg train loss per sample:\",total_loss_train.item()/len(trainset))\n",
    "    model.eval()\n",
    "    total_loss_eval = 0\n",
    "    for batch in testset:\n",
    "        with torch.no_grad():\n",
    "            input_ids, input_token_type_ids, input_attention_mask ,labels= batch\n",
    "            output_eval = model(input_ids,token_type_ids = input_token_type_ids,attention_mask =input_attention_mask, labels=labels)\n",
    "            loss = output_eval[0]\n",
    "            total_loss_eval+=loss\n",
    "    print(\"    Avg eval loss per sample:\",total_loss_eval.item()/len(evalset))\n",
    "\n",
    "pred_label.to(\"cpu\")\n",
    "\n",
    "len(dataset_train)\n",
    "\n",
    "data"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM71M66YdDhJtz4zL0bRCqo",
   "collapsed_sections": [],
   "name": "Untitled11.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
