{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled11.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM71M66YdDhJtz4zL0bRCqo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"8YzFB8sn7coV","executionInfo":{"status":"ok","timestamp":1614858323090,"user_tz":-540,"elapsed":4569,"user":{"displayName":"童祺俊","photoUrl":"","userId":"10868036175973256652"}}},"source":["import pandas as pd\n","import torch\n","from sklearn.preprocessing import LabelEncoder\n","from transformers import AutoTokenizer, BertForSequenceClassification, AdamW\n","from tqdm import trange\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import dataloader"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"u0_62rny8kT1","executionInfo":{"status":"ok","timestamp":1614858323090,"user_tz":-540,"elapsed":4566,"user":{"displayName":"童祺俊","photoUrl":"","userId":"10868036175973256652"}}},"source":["device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"3fZo2gnW7gBk","executionInfo":{"status":"ok","timestamp":1614858323091,"user_tz":-540,"elapsed":4564,"user":{"displayName":"童祺俊","photoUrl":"","userId":"10868036175973256652"}}},"source":["df=pd.read_csv(\"atcoder_problem_tag_dataset.csv\")"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"oT-8BeJW7jZL","executionInfo":{"status":"ok","timestamp":1614858323091,"user_tz":-540,"elapsed":4562,"user":{"displayName":"童祺俊","photoUrl":"","userId":"10868036175973256652"}}},"source":["df=df[df[\"problem_texts\"].notna()].reset_index()\n","df[\"concatenate_texts\"]=df[\"problem_texts\"]+df[\"constraints\"]+df[\"input_texts\"]+df[\"output_texts\"]\n","le = LabelEncoder()\n","df[\"tag\"] = le.fit_transform(df[\"tag\"])"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"0ZkMsGzU7myl","executionInfo":{"status":"ok","timestamp":1614858323091,"user_tz":-540,"elapsed":4559,"user":{"displayName":"童祺俊","photoUrl":"","userId":"10868036175973256652"}}},"source":["df_copy = df.copy()\n","df_train = df_copy.sample(frac=0.8, random_state=0)\n","df_test = df_copy.drop(df_train.index).reset_index()\n","df_train = df_train.reset_index()"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R_3ry9WF7m6T","executionInfo":{"status":"ok","timestamp":1614858331053,"user_tz":-540,"elapsed":12518,"user":{"displayName":"童祺俊","photoUrl":"","userId":"10868036175973256652"}},"outputId":"f5dcaefa-ee20-478a-b3ab-8ca287da141c"},"source":["tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n","model = BertForSequenceClassification.from_pretrained('bert-base-cased', return_dict=True,num_labels=14)\n","model.to(device)\n","optimizer = AdamW(model.parameters(), lr=1e-5)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"-Z6hwquo7m_e","executionInfo":{"status":"ok","timestamp":1614858331054,"user_tz":-540,"elapsed":12513,"user":{"displayName":"童祺俊","photoUrl":"","userId":"10868036175973256652"}}},"source":["class DataSet:\n","    def __init__(self,df,tokenizer,input_col_name,output_col_name):\n","        self.X = tokenizer.batch_encode_plus(df[\"concatenate_texts\"].tolist(),truncation=True,max_length=512,pad_to_max_length=True, add_special_tokens=True,return_tensors='pt')\n","        self.y = torch.tensor(df[output_col_name])\n","    \n","    def __len__(self):\n","        return len(self.y)\n","\n","    def __getitem__(self,index):\n","        return self.X[\"input_ids\"][index].to(device),self.X[\"token_type_ids\"][index].to(device), self.X[\"attention_mask\"][index].to(device), self.y[index].to(device)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AI40L3cv7unN","executionInfo":{"status":"ok","timestamp":1614858332390,"user_tz":-540,"elapsed":13845,"user":{"displayName":"童祺俊","photoUrl":"","userId":"10868036175973256652"}},"outputId":"3c793121-a92a-4181-d53d-f2e5ed45f4d5"},"source":["dataset_train = DataSet(df_train,tokenizer,\"concatenate_texts\",\"tag\")\n","dataset_test = DataSet(df_train,tokenizer,\"concatenate_texts\",\"tag\")\n","trainset = dataloader.DataLoader(dataset = dataset_train, shuffle=True, batch_size = 8)\n","testset = dataloader.DataLoader(dataset = dataset_test, shuffle=True, batch_size = 8)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2155: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X95yn3kR7xCj","outputId":"1c468589-e541-49f1-f692-af57ece388b0"},"source":["for epoch in range(1,101):\n","    model.train()\n","    total_loss_train=0\n","    for batch in trainset:\n","        optimizer.zero_grad()\n","        input_ids, input_token_type_ids, input_attention_mask ,labels= batch \n","        output = model(input_ids,token_type_ids = input_token_type_ids,attention_mask =input_attention_mask, labels=labels)\n","        loss = output[0]\n","        loss.backward()\n","        optimizer.step()\n","        model.zero_grad()\n","        total_loss_train+=loss\n","        print(\"epoch\",epoch)\n","        print(\"    Avg train loss per sample:\",total_loss_train.item()/len(trainset))   "],"execution_count":null,"outputs":[{"output_type":"stream","text":["epoch 1\n","    Avg train loss per sample: 0.01605034161763019\n","epoch 1\n","    Avg train loss per sample: 0.03324786438999406\n","epoch 1\n","    Avg train loss per sample: 0.04970192621989423\n","epoch 1\n","    Avg train loss per sample: 0.06664139391428017\n","epoch 1\n","    Avg train loss per sample: 0.08219859686242528\n","epoch 1\n","    Avg train loss per sample: 0.09840567715196724\n","epoch 1\n","    Avg train loss per sample: 0.11551318111189876\n","epoch 1\n","    Avg train loss per sample: 0.1312180714434888\n","epoch 1\n","    Avg train loss per sample: 0.14698680050401802\n","epoch 1\n","    Avg train loss per sample: 0.1622063165687653\n","epoch 1\n","    Avg train loss per sample: 0.18003878535994564\n","epoch 1\n","    Avg train loss per sample: 0.19517783659050264\n","epoch 1\n","    Avg train loss per sample: 0.2114365818988846\n","epoch 1\n","    Avg train loss per sample: 0.22818046018301721\n","epoch 1\n","    Avg train loss per sample: 0.2439439153096762\n","epoch 1\n","    Avg train loss per sample: 0.25854816206966535\n","epoch 1\n","    Avg train loss per sample: 0.2738305287188794\n","epoch 1\n","    Avg train loss per sample: 0.28996812291892177\n","epoch 1\n","    Avg train loss per sample: 0.30476043885012705\n","epoch 1\n","    Avg train loss per sample: 0.3200904202748494\n","epoch 1\n","    Avg train loss per sample: 0.33525988567306336\n","epoch 1\n","    Avg train loss per sample: 0.3509319948862834\n","epoch 1\n","    Avg train loss per sample: 0.36744184379118033\n","epoch 1\n","    Avg train loss per sample: 0.3824327997414462\n","epoch 1\n","    Avg train loss per sample: 0.3972287465290851\n","epoch 1\n","    Avg train loss per sample: 0.4142141227262566\n","epoch 1\n","    Avg train loss per sample: 0.42955986850232963\n","epoch 1\n","    Avg train loss per sample: 0.4450616032244211\n","epoch 1\n","    Avg train loss per sample: 0.45996309762977694\n","epoch 1\n","    Avg train loss per sample: 0.4736955481839467\n","epoch 1\n","    Avg train loss per sample: 0.48850227264036616\n","epoch 1\n","    Avg train loss per sample: 0.50419262805617\n","epoch 1\n","    Avg train loss per sample: 0.520639350615352\n","epoch 1\n","    Avg train loss per sample: 0.5351705436246941\n","epoch 1\n","    Avg train loss per sample: 0.5502914980233434\n","epoch 1\n","    Avg train loss per sample: 0.565032958984375\n","epoch 1\n","    Avg train loss per sample: 0.578174131462373\n","epoch 1\n","    Avg train loss per sample: 0.5930291600974209\n","epoch 1\n","    Avg train loss per sample: 0.6065874214631966\n","epoch 1\n","    Avg train loss per sample: 0.622082032353045\n","epoch 1\n","    Avg train loss per sample: 0.6377034474568195\n","epoch 1\n","    Avg train loss per sample: 0.6500951927828501\n","epoch 1\n","    Avg train loss per sample: 0.6640388304928699\n","epoch 1\n","    Avg train loss per sample: 0.6794892276626036\n","epoch 1\n","    Avg train loss per sample: 0.6948677430669945\n","epoch 1\n","    Avg train loss per sample: 0.7098450258553747\n","epoch 1\n","    Avg train loss per sample: 0.723247757877212\n","epoch 1\n","    Avg train loss per sample: 0.7358442559299698\n","epoch 1\n","    Avg train loss per sample: 0.7494652989399002\n","epoch 1\n","    Avg train loss per sample: 0.7626074365822666\n","epoch 1\n","    Avg train loss per sample: 0.7779787362339985\n","epoch 1\n","    Avg train loss per sample: 0.7939181040568524\n","epoch 1\n","    Avg train loss per sample: 0.8098366059452654\n","epoch 1\n","    Avg train loss per sample: 0.8246175926851939\n","epoch 1\n","    Avg train loss per sample: 0.8365709235869259\n","epoch 1\n","    Avg train loss per sample: 0.8517603127353163\n","epoch 1\n","    Avg train loss per sample: 0.8666382755141661\n","epoch 1\n","    Avg train loss per sample: 0.8808212280273438\n","epoch 1\n","    Avg train loss per sample: 0.8962635821606739\n","epoch 1\n","    Avg train loss per sample: 0.9097563961902296\n","epoch 1\n","    Avg train loss per sample: 0.9245030966149755\n","epoch 1\n","    Avg train loss per sample: 0.9395177450524755\n","epoch 1\n","    Avg train loss per sample: 0.9529702060193901\n","epoch 1\n","    Avg train loss per sample: 0.9686133143413498\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"52BX1Dkp9TNY","executionInfo":{"status":"aborted","timestamp":1614858332392,"user_tz":-540,"elapsed":13835,"user":{"displayName":"童祺俊","photoUrl":"","userId":"10868036175973256652"}}},"source":["for batch in testset:\n","    input_ids, input_token_type_ids, input_attention_mask ,labels= batch \n","    output = model(input_ids,token_type_ids = input_token_type_ids,attention_mask =input_attention_mask, labels=labels)\n","    pred_label = torch.argmax(output[1],axis=0)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4Ml-GiLt-jU-","executionInfo":{"status":"aborted","timestamp":1614858332393,"user_tz":-540,"elapsed":13833,"user":{"displayName":"童祺俊","photoUrl":"","userId":"10868036175973256652"}}},"source":["pred_label.to(\"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"70tIvUqm-kf8","executionInfo":{"status":"aborted","timestamp":1614858332393,"user_tz":-540,"elapsed":13831,"user":{"displayName":"童祺俊","photoUrl":"","userId":"10868036175973256652"}}},"source":["len(dataset_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"re_5unlT-8lX","executionInfo":{"status":"aborted","timestamp":1614858332393,"user_tz":-540,"elapsed":13828,"user":{"displayName":"童祺俊","photoUrl":"","userId":"10868036175973256652"}}},"source":["data"],"execution_count":null,"outputs":[]}]}