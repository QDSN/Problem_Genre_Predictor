{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 4569,
     "status": "ok",
     "timestamp": 1614858323090,
     "user": {
      "displayName": "童祺俊",
      "photoUrl": "",
      "userId": "10868036175973256652"
     },
     "user_tz": -540
    },
    "id": "8YzFB8sn7coV"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification, AdamW\n",
    "from transformers import AlbertTokenizer, AlbertForSequenceClassification\n",
    "from tqdm import trange\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 4566,
     "status": "ok",
     "timestamp": 1614858323090,
     "user": {
      "displayName": "童祺俊",
      "photoUrl": "",
      "userId": "10868036175973256652"
     },
     "user_tz": -540
    },
    "id": "u0_62rny8kT1"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 4564,
     "status": "ok",
     "timestamp": 1614858323091,
     "user": {
      "displayName": "童祺俊",
      "photoUrl": "",
      "userId": "10868036175973256652"
     },
     "user_tz": -540
    },
    "id": "3fZo2gnW7gBk"
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"../data/atcoder_problem_tag_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 4562,
     "status": "ok",
     "timestamp": 1614858323091,
     "user": {
      "displayName": "童祺俊",
      "photoUrl": "",
      "userId": "10868036175973256652"
     },
     "user_tz": -540
    },
    "id": "oT-8BeJW7jZL"
   },
   "outputs": [],
   "source": [
    "df=df[df[\"problem_texts\"].notna()].reset_index()\n",
    "df[\"concatenate_texts\"]=df[\"problem_texts\"]+df[\"constraints\"]+df[\"input_texts\"]+df[\"output_texts\"]\n",
    "le = LabelEncoder()\n",
    "df[\"tag\"] = le.fit_transform(df[\"tag\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 4559,
     "status": "ok",
     "timestamp": 1614858323091,
     "user": {
      "displayName": "童祺俊",
      "photoUrl": "",
      "userId": "10868036175973256652"
     },
     "user_tz": -540
    },
    "id": "0ZkMsGzU7myl"
   },
   "outputs": [],
   "source": [
    "df_copy = df.copy()\n",
    "df_train = df_copy.sample(frac=0.8, random_state=0)\n",
    "df_eval = df_copy.drop(df_train.index).reset_index()\n",
    "df_train = df_train.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12518,
     "status": "ok",
     "timestamp": 1614858331053,
     "user": {
      "displayName": "童祺俊",
      "photoUrl": "",
      "userId": "10868036175973256652"
     },
     "user_tz": -540
    },
    "id": "R_3ry9WF7m6T",
    "outputId": "f5dcaefa-ee20-478a-b3ab-8ca287da141c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-cased', return_dict=True,num_labels=14)\n",
    "#tokenizer = AutoTokenizer.from_pretrained('albert-base-v2')\n",
    "#model = AlbertForSequenceClassification.from_pretrained('albert-base-v2', return_dict=True,num_labels=14)\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 12513,
     "status": "ok",
     "timestamp": 1614858331054,
     "user": {
      "displayName": "童祺俊",
      "photoUrl": "",
      "userId": "10868036175973256652"
     },
     "user_tz": -540
    },
    "id": "-Z6hwquo7m_e"
   },
   "outputs": [],
   "source": [
    "class DataSet:\n",
    "    def __init__(self,df,tokenizer,input_col_name,output_col_name):\n",
    "        self.X = tokenizer.batch_encode_plus(df[\"concatenate_texts\"].tolist(),truncation=True,max_length=512,pad_to_max_length=True, add_special_tokens=True,return_tensors='pt')\n",
    "        self.y = torch.tensor(df[output_col_name])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return self.X[\"input_ids\"][index].to(device),self.X[\"token_type_ids\"][index].to(device), self.X[\"attention_mask\"][index].to(device), self.y[index].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13845,
     "status": "ok",
     "timestamp": 1614858332390,
     "user": {
      "displayName": "童祺俊",
      "photoUrl": "",
      "userId": "10868036175973256652"
     },
     "user_tz": -540
    },
    "id": "AI40L3cv7unN",
    "outputId": "3c793121-a92a-4181-d53d-f2e5ed45f4d5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2155: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "dataset_train = DataSet(df_train,tokenizer,\"concatenate_texts\",\"tag\")\n",
    "dataset_eval = DataSet(df_train,tokenizer,\"concatenate_texts\",\"tag\")\n",
    "trainset = dataloader.DataLoader(dataset = dataset_train, shuffle=True, batch_size = 8)\n",
    "evalset = dataloader.DataLoader(dataset = dataset_eval, shuffle=True, batch_size = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X95yn3kR7xCj",
    "outputId": "1c468589-e541-49f1-f692-af57ece388b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "    Avg train loss per sample: 0.01605034161763019\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.03324786438999406\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.04970192621989423\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.06664139391428017\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.08219859686242528\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.09840567715196724\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.11551318111189876\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.1312180714434888\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.14698680050401802\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.1622063165687653\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.18003878535994564\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.19517783659050264\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.2114365818988846\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.22818046018301721\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.2439439153096762\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.25854816206966535\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.2738305287188794\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.28996812291892177\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.30476043885012705\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.3200904202748494\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.33525988567306336\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.3509319948862834\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.36744184379118033\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.3824327997414462\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.3972287465290851\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.4142141227262566\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.42955986850232963\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.4450616032244211\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.45996309762977694\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.4736955481839467\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.48850227264036616\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.50419262805617\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.520639350615352\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.5351705436246941\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.5502914980233434\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.565032958984375\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.578174131462373\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.5930291600974209\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.6065874214631966\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.622082032353045\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.6377034474568195\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.6500951927828501\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.6640388304928699\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.6794892276626036\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.6948677430669945\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.7098450258553747\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.723247757877212\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.7358442559299698\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.7494652989399002\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.7626074365822666\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.7779787362339985\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.7939181040568524\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.8098366059452654\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.8246175926851939\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.8365709235869259\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.8517603127353163\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.8666382755141661\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.8808212280273438\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.8962635821606739\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.9097563961902296\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.9245030966149755\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.9395177450524755\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.9529702060193901\n",
      "epoch 1\n",
      "    Avg train loss per sample: 0.9686133143413498\n"
     ]
    }
   ],
   "source": [
    "for epoch in trange(1,51):\n",
    "    model.train()\n",
    "    total_loss_train=0\n",
    "    for batch in trainset:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids, input_token_type_ids, input_attention_mask ,labels= batch \n",
    "        output = model(input_ids,token_type_ids = input_token_type_ids,attention_mask =input_attention_mask, labels=labels)\n",
    "        loss = output[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "        total_loss_train+=loss\n",
    "    print(\"epoch\",epoch)\n",
    "    print(\"    Avg train loss per sample:\",total_loss_train.item()/len(trainset))\n",
    "    model.eval()\n",
    "    total_loss_eval = 0\n",
    "    for batch in evalset:\n",
    "        with torch.no_grad():\n",
    "            input_ids, input_token_type_ids, input_attention_mask ,labels= batch\n",
    "            output_eval = model(input_ids,token_type_ids = input_token_type_ids,attention_mask =input_attention_mask, labels=labels)\n",
    "            loss = output_eval[0]\n",
    "            total_loss_eval+=loss\n",
    "    print(\"    Avg eval loss per sample:\",total_loss_eval.item()/len(evalset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13835,
     "status": "aborted",
     "timestamp": 1614858332392,
     "user": {
      "displayName": "童祺俊",
      "photoUrl": "",
      "userId": "10868036175973256652"
     },
     "user_tz": -540
    },
    "id": "52BX1Dkp9TNY"
   },
   "outputs": [],
   "source": [
    "for batch in testset:\n",
    "    input_ids, input_token_type_ids, input_attention_mask ,labels= batch \n",
    "    output = model(input_ids,token_type_ids = input_token_type_ids,attention_mask =input_attention_mask, labels=labels)\n",
    "    pred_label = torch.argmax(output[1],axis=0)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM71M66YdDhJtz4zL0bRCqo",
   "collapsed_sections": [],
   "name": "Untitled11.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
